# -*- coding: utf-8 -*-
"""korean_essay_eval_transfromer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AjNQfkAECbwo6xmCfrKDxi8wS_hikGzq

# [AB] AI Writing Supporter 프로젝트

## 0. 모델링 코드

## 1. 환경설정

### 1-1. 라이브러리 및 폰트
"""


# Google Colab 및 데이터처리 라이브러리
import pandas as pd
import numpy as np
import os
from tqdm import tqdm

# 자연어처리 라이브러리
import sentencepiece as spm
from konlpy.tag import Okt

# 모델링 라이브러리
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import plot_model
from tensorflow.keras import backend as K
from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers
from tensorflow.keras.layers import Layer

# 성능평가 라이브러리
from sklearn.metrics import mean_squared_error, mean_absolute_error

"""### 1-2. 버전 확인"""

print(tf.__version__)
print(pd.__version__)
print(np.__version__)

"""### 1-3. GPU & Memory 확인"""

"""## 2. 데이터 전처리

### 2-1. 데이터 로드
"""


# data를 저장할 폴더 입니다. 환경에 맞게 수정 하세요.
# data_dir = "/content/drive/My Drive/2022 AI SW 온라인 실무 교육 사례 대회/data_reg/train/"
# test_dir = "/content/drive/My Drive/2022 AI SW 온라인 실무 교육 사례 대회/data_reg/test/"

"""### 2-2. 데이터 EDA
- `data_visualization.ipynb`

### 2-3. 데이터 전처리
- 불용어 제거 및 문장 인코딩

#### 2-2-1. SentencePiece
"""
'''
def sentencepiece_preprocessing(train, test, vocab_size, maxlen_subject, maxlen_prompt, maxlen_paragraph):

    vocab_file = f"{data_dir[:51]}vocab/kowiki.model"
    vocab = spm.SentencePieceProcessor()
    vocab.load(vocab_file)

    y_col = train.columns.drop(["essay_main_subject", "essay_prompt", "paragraph", 'essay_scoreT_org', 'essay_scoreT_cont', 'essay_scoreT_exp'])
    
    x_train = train[["essay_main_subject", "essay_prompt", "paragraph"]]
    y_train = train[y_col]
    x_test = test[["essay_main_subject", "essay_prompt", "paragraph"]]
    y_test = test[y_col]
    
    for l in tqdm(range(len(x_train))):
        x_train['essay_main_subject'][l] = vocab.encode_as_ids(x_train['essay_main_subject'][l])
        x_train['essay_prompt'][l] = vocab.encode_as_ids(x_train['essay_prompt'][l])
        x_train['paragraph'][l] = vocab.encode_as_ids(x_train['paragraph'][l])

    for l in tqdm(range(len(x_test))):
        x_test['essay_main_subject'][l] = vocab.encode_as_ids(x_test['essay_main_subject'][l])
        x_test['essay_prompt'][l] = vocab.encode_as_ids(x_test['essay_prompt'][l])
        x_test['paragraph'][l] = vocab.encode_as_ids(x_test['paragraph'][l])

    # 패딩
    x_train1 = keras.preprocessing.sequence.pad_sequences(x_train['essay_main_subject'], maxlen=maxlen_subject)
    x_train2 = keras.preprocessing.sequence.pad_sequences(x_train['essay_prompt'], maxlen=maxlen_prompt)
    x_train3 = keras.preprocessing.sequence.pad_sequences(x_train['paragraph'], maxlen=maxlen_paragraph)
    x_test1 = keras.preprocessing.sequence.pad_sequences(x_test['essay_main_subject'], maxlen=maxlen_subject)
    x_test2 = keras.preprocessing.sequence.pad_sequences(x_test['essay_prompt'], maxlen=maxlen_prompt)
    x_test3 = keras.preprocessing.sequence.pad_sequences(x_test['paragraph'], maxlen=maxlen_paragraph)

    return [x_train1, x_train2, x_train3], y_train, [x_test1, x_test2, x_test3], y_test, 0

"""#### 2-2-2. KoNLPy
- Okt, Hannanum, Kkma, Komoran 사용 가능
"""

def konlpy_preprocessing(train, test, vocab_size, maxlen_subject, maxlen_prompt, maxlen_paragraph):

    okt = Okt()
    # stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']
    y_col = train.columns.drop(["essay_main_subject", "essay_prompt", "paragraph", 'essay_scoreT_org', 'essay_scoreT_cont', 'essay_scoreT_exp'])
    
    x_train = train[["essay_main_subject", "essay_prompt", "paragraph"]]
    y_train = train[y_col]
    x_test = test[["essay_main_subject", "essay_prompt", "paragraph"]]
    y_test = test[y_col]
    
    for l in tqdm(range(len(x_train))):
        x_train['essay_main_subject'][l] = okt.morphs(x_train['essay_main_subject'][l], stem=True)
        x_train['essay_prompt'][l] = okt.morphs(x_train['essay_prompt'][l], stem=True)
        x_train['paragraph'][l] = okt.morphs(x_train['paragraph'][l], stem=True)

    for l in tqdm(range(len(x_test))):
        x_test['essay_main_subject'][l] = okt.morphs(x_test['essay_main_subject'][l], stem=True)
        x_test['essay_prompt'][l] = okt.morphs(x_test['essay_prompt'][l], stem=True)
        x_test['paragraph'][l] = okt.morphs(x_test['paragraph'][l], stem=True)

    tokenizer1 = Tokenizer() # 정수 인코딩
    tokenizer1.fit_on_texts(x_train['essay_main_subject']) # tokenizer.word_index로 확인 가능
    tokenizer2 = Tokenizer() # 정수 인코딩
    tokenizer2.fit_on_texts(x_train['essay_prompt']) # tokenizer.word_index로 확인 가능
    tokenizer3 = Tokenizer() # 정수 인코딩
    tokenizer3.fit_on_texts(x_train['paragraph']) # tokenizer.word_index로 확인 가능

    x_train['essay_main_subject'] = tokenizer1.texts_to_sequences(x_train['essay_main_subject'])
    x_train['essay_prompt'] = tokenizer2.texts_to_sequences(x_train['essay_prompt'])
    x_train['paragraph'] = tokenizer3.texts_to_sequences(x_train['paragraph'])
    x_test['essay_main_subject'] = tokenizer1.texts_to_sequences(x_test['essay_main_subject'])
    x_test['essay_prompt'] = tokenizer2.texts_to_sequences(x_test['essay_prompt'])
    x_test['paragraph'] = tokenizer3.texts_to_sequences(x_test['paragraph'])

    # 패딩
    x_train1 = keras.preprocessing.sequence.pad_sequences(x_train['essay_main_subject'], maxlen=maxlen_subject)
    x_train2 = keras.preprocessing.sequence.pad_sequences(x_train['essay_prompt'], maxlen=maxlen_prompt)
    x_train3 = keras.preprocessing.sequence.pad_sequences(x_train['paragraph'], maxlen=maxlen_paragraph)
    x_test1 = keras.preprocessing.sequence.pad_sequences(x_test['essay_main_subject'], maxlen=maxlen_subject)
    x_test2 = keras.preprocessing.sequence.pad_sequences(x_test['essay_prompt'], maxlen=maxlen_prompt)
    x_test3 = keras.preprocessing.sequence.pad_sequences(x_test['paragraph'], maxlen=maxlen_paragraph)

    return [x_train1, x_train2, x_train3], y_train, [x_test1, x_test2, x_test3], y_test, [tokenizer1, tokenizer2, tokenizer3]

def data_preprocessing(method, train, test, maxlen):
    
    vocab_size = 20000 

    if method == 'SentencePiece':
        return sentencepiece_preprocessing(train, test, vocab_size, maxlen[0], maxlen[1], maxlen[2])
    
    elif method == 'konlpy':
        return konlpy_preprocessing(train, test, vocab_size, maxlen[0], maxlen[1], maxlen[2])
    
    else:
        print('None')
'''
"""## 3. 모델

### 3-1. 모델

#### 3-1-1. LSTM

#### 3-1-2. Attention
"""

class Attention(keras.layers.Layer):
    def __init__(self, step_dim,
                 W_regularizer=None, b_regularizer=None,
                 W_constraint=None, b_constraint=None,
                 bias=True, **kwargs):
        self.supports_masking = True
        self.init = initializers.get('glorot_uniform')

        self.W_regularizer = regularizers.get(W_regularizer)
        self.b_regularizer = regularizers.get(b_regularizer)

        self.W_constraint = constraints.get(W_constraint)
        self.b_constraint = constraints.get(b_constraint)

        self.bias = bias
        self.step_dim = step_dim
        self.features_dim = 0
        super(Attention, self).__init__(**kwargs)

    def get_config(self):
        config = super().get_config().copy()
        config.update({
          'W_regularizer': self.W_regularizer,
          'b_regularizer': self.b_regularizer,
          'W_constraint': self.W_constraint,
          'b_constraint': self.b_constraint,
          'bias': self.bias
        })
        return config

      
    def build(self, input_shape):
        assert len(input_shape) == 3

        self.W = self.add_weight(shape=(input_shape[-1],),
                                 initializer=self.init,
                                 name='{}_W'.format(self.name),
                                 regularizer=self.W_regularizer,
                                 constraint=self.W_constraint)
        self.features_dim = input_shape[-1]

        if self.bias:
            self.b = self.add_weight(shape=(input_shape[1],),
                                     initializer='zero',
                                     name='{}_b'.format(self.name),
                                     regularizer=self.b_regularizer,
                                     constraint=self.b_constraint)
        else:
            self.b = None

        self.built = True

    def compute_mask(self, input, input_mask=None):
        return None

    def call(self, x, mask=None):
        features_dim = self.features_dim
        step_dim = self.step_dim

        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),
                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))

        if self.bias:
            eij += self.b

        eij = K.tanh(eij)

        a = K.exp(eij)

        if mask is not None:
            a *= K.cast(mask, K.floatx())

        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())

        a = K.expand_dims(a)
        weighted_input = x * a
        return K.sum(weighted_input, axis=1)

    def compute_output_shape(self, input_shape):
        return input_shape[0],  self.features_dim

"""#### 3-1-3. Transformer
- Transformer 블록을 레이어로 사용
"""

class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = keras.Sequential(
            [layers.Dense(ff_dim, activation="relu"), layers.Dense(embed_dim),]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

"""- 임베딩 레이어"""

class TokenAndPositionEmbedding(layers.Layer):
    def __init__(self, maxlen, vocab_size, embed_dim):
        super(TokenAndPositionEmbedding, self).__init__()
        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)

    def call(self, x):
        maxlen = tf.shape(x)[-1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions

"""### 3-2. 모델 구현 함수"""

def make_model(model_name, maxlen, vocab_size):
    embed_dim = 32  # Embedding size for each token

    y_cols = ['essay_scoreT_org_0', 'essay_scoreT_org_1', 'essay_scoreT_org_2',
                'essay_scoreT_org_3', 'essay_scoreT_cont_0', 'essay_scoreT_cont_1',
                'essay_scoreT_cont_2', 'essay_scoreT_cont_3', 'essay_scoreT_exp_0',
                'essay_scoreT_exp_1', 'essay_scoreT_exp_2']

    outputs = []

    if model_name == 'LSTM': 
        # 입력 레이어
        inputs_subject = layers.Input(shape=(maxlen[0],))
        inputs_prompt = layers.Input(shape=(maxlen[1],))
        inputs_paragraph = layers.Input(shape=(maxlen[2],))

        # 임베딩 및 LSTM 레이어 설정
        x1 = layers.Embedding(vocab_size, embed_dim)(inputs_subject)
        x1 = tf.keras.layers.LSTM(64, return_sequences=True)(x1)
        x1 = layers.GlobalAveragePooling1D()(x1)

        x2 = layers.Embedding(vocab_size, embed_dim)(inputs_prompt)
        x2 = tf.keras.layers.LSTM(64, return_sequences=True)(x2)
        x2 = layers.GlobalAveragePooling1D()(x2)

        x3 = layers.Embedding(vocab_size, embed_dim)(inputs_paragraph)
        x3 = tf.keras.layers.LSTM(64, return_sequences=True)(x3)
        x1 = layers.GlobalAveragePooling1D()(x3)

        # 레이어 결합
        c = layers.concatenate([x1, x2, x3])
        c = layers.Dropout(0.1)(c)
        c = layers.Dense(32, activation="relu")(c)
        c = layers.Dropout(0.1)(c)

        # 출력 레이어
        for y_col in y_cols:
            outputs.append(layers.Dense(1, name=y_col)(c))
    
    elif model_name == 'Attention':
        # 입력 레이어
        inputs_subject = layers.Input(shape=(maxlen[0],))
        inputs_prompt = layers.Input(shape=(maxlen[1],))
        inputs_paragraph = layers.Input(shape=(maxlen[2],))

        # 임베딩 및 Attention 레이어 설정
        x1 = layers.Embedding(vocab_size, embed_dim)(inputs_subject)
        x1 = Attention(maxlen[0])(x1)
        
        x2 = layers.Embedding(vocab_size, embed_dim)(inputs_prompt)
        x2 = Attention(maxlen[1])(x2)

        x3 = layers.Embedding(vocab_size, embed_dim)(inputs_paragraph)
        x3 = Attention(maxlen[2])(x3)

        # 레이어 결합
        c = layers.concatenate([x1, x2, x3])
        c = layers.Dense(64, activation="relu")(c)
        c = layers.Dropout(0.1)(c)

        # 출력 레이어
        for y_col in y_cols:
            outputs.append(layers.Dense(1, name=y_col)(c))

    elif model_name == 'Transformer':
        num_heads = 4  # Number of attention heads
        ff_dim = 32  # Hidden layer size in feed forward network inside transformer

        # 입력 레이어
        inputs_subject = layers.Input(shape=(maxlen[0],))
        inputs_prompt = layers.Input(shape=(maxlen[1],))
        inputs_paragraph = layers.Input(shape=(maxlen[2],))

        # 임베딩 및 Transformer block 레이어 설정
        embedding_layer1 = TokenAndPositionEmbedding(maxlen[0], vocab_size, embed_dim)
        x1 = embedding_layer1(inputs_subject)
        transformer_block1 = TransformerBlock(embed_dim, num_heads, ff_dim)

        x1 = transformer_block1(x1)
        x1 = layers.GlobalAveragePooling1D()(x1)
        x1 = layers.Dropout(0.1)(x1)

        embedding_layer2 = TokenAndPositionEmbedding(maxlen[1], vocab_size, embed_dim)
        x2 = embedding_layer2(inputs_prompt)
        transformer_block2 = TransformerBlock(embed_dim, num_heads, ff_dim)

        x2 = transformer_block2(x2)
        x2 = layers.GlobalAveragePooling1D()(x2)
        x2 = layers.Dropout(0.1)(x2)

        embedding_layer3 = TokenAndPositionEmbedding(maxlen[2], vocab_size, embed_dim)
        x3 = embedding_layer3(inputs_paragraph)
        transformer_block3 = TransformerBlock(embed_dim, num_heads, ff_dim)

        x3 = transformer_block3(x3)
        x3 = layers.GlobalAveragePooling1D()(x3)
        x3 = layers.Dropout(0.1)(x3)

        # 레이어 결합
        c = layers.concatenate([x1, x2, x3])
        c = layers.Dense(64, activation="relu")(c)
        c = layers.Dropout(0.1)(c)
        c = layers.Dense(32, activation="relu")(c)
        c = layers.Dropout(0.1)(c)

        # 출력 레이어
        for y_col in y_cols:
            outputs.append(layers.Dense(1, name=y_col)(c))

    else:
        print("모델을 지정해주세요.")

    return keras.Model(inputs=[inputs_subject, inputs_prompt, inputs_paragraph], outputs=outputs)

"""### 3-3. 모델 실행 함수"""

def model_run(model, x_train, y_train, checkpoint_path, optimizer="adam"):
  epochs = 500
  batch_size = 32

  callbacks = [
    tf.keras.callbacks.ModelCheckpoint(
        checkpoint_path, save_weights_only=True, verbose=1, save_best_only=True
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor="val_loss", factor=0.5, patience=5, min_lr=0.0001, mode='min'
    ),
    tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=10, verbose=1),
  ]
  model.compile(
    optimizer=optimizer,
    loss='mse', 
    metrics=['mae']
  )
  history = model.fit(
    x_train,
    y_train,
    batch_size=batch_size,
    epochs=epochs,
    callbacks=callbacks,
    validation_split=0.2,
    verbose=1,
  )

"""### 3-4. 모델 학습"""

def preprocessing_and_modeling(essay_type, model_name, preprocessing_method, model_algorithm, maxlen, vocab_size):
    train = pd.read_csv(data_dir+'train_'+essay_type+'.csv')
    test = pd.read_csv(test_dir+'test_'+essay_type+'.csv')
    print(train.shape, test.shape)
    print(train.head())

    train_pp = train.reset_index().drop('index', axis=1)
    test_pp = test.reset_index().drop('index', axis=1)

    x_train, y_train, x_test, y_test, tokenizer_data = data_preprocessing(preprocessing_method, train_pp, test_pp, maxlen)
    # print(x_train[0].shape, x_train[1].shape, x_train[2].shape, y_train[0].shape, y_train[1].shape, y_train[2].shape, x_test.shape, y_test.shape)

    # 모델 저장 경로
    model_dir = data_dir[:51]+model_name +'/'
    if not os.path.exists(model_dir):
        os.mkdir(model_dir)

    # 체크포인트 파일 저장 경로
    checkpoint_path = model_dir+"checkpoints_"+model_name+".ckpt"

    model = make_model(model_algorithm, maxlen, vocab_size)
    print(model.summary())

    plot_model(model, to_file=model_name+'.png')
    plot_model(model, to_file=model_name+'_shapes.png', show_shapes=True)

    y_train_list = [y_train['essay_scoreT_org_0'], y_train['essay_scoreT_org_1'], y_train['essay_scoreT_org_2'],
                y_train['essay_scoreT_org_3'], y_train['essay_scoreT_cont_0'], y_train['essay_scoreT_cont_1'],
                y_train['essay_scoreT_cont_2'], y_train['essay_scoreT_cont_3'], y_train['essay_scoreT_exp_0'],
                y_train['essay_scoreT_exp_1'], y_train['essay_scoreT_exp_2']]

    model_run(  model,
                x_train, 
                y_train_list, 
                checkpoint_path, 
                optimizer="adam")

# essay_types = ['주장', '설명글', '찬성반대', '대안제시', '글짓기']
# for e_type in essay_types:
#     preprocessing_and_modeling(e_type, 'model_transformer_'+e_type, 'SentencePiece', 'Transformer', [20, 200, 2000], 20000)



"""## 4. 성능 평가

### 4-1. 성능 평가 함수
"""

def evalution(e_type, model_name, preprocessing_method, model_algorithm, maxlen, vocab_size):
    train = pd.read_csv(data_dir+'train_'+e_type+'.csv')
    test = pd.read_csv(test_dir+'test_'+e_type+'.csv')
    print(train.shape, test.shape)
    print(train.head())

    train_pp = train.reset_index().drop('index', axis=1)
    test_pp = test.reset_index().drop('index', axis=1)

    x_train, y_train, x_test, y_test, tokenizer_data = data_preprocessing(preprocessing_method, train_pp, test_pp, maxlen)

    # 모델 저장 경로
    model_dir = data_dir[:51]+model_name +'/'
    if not os.path.exists(model_dir):
        os.mkdir(model_dir)

    # 체크포인트 파일 저장 경로
    checkpoint_path = model_dir+"checkpoints_"+model_name+".ckpt"

    model = make_model(model_algorithm, maxlen, vocab_size)
    model.load_weights(checkpoint_path)
    model.compile(
        optimizer="adam",
        loss="mse",
        metrics=["mae"],
    )

    # data_pre = model.predict(x_test)
    # print(data_pre)
    pred_z = model.predict(x_test)
    pred_z = np.asarray(pred_z).astype('float32')
    y_col = y_test.columns
    
    eval = []

    print(e_type)
    for i in range(11):
        print(str(i+1)+'번째 평가항목')
        mse = mean_squared_error(y_test[y_col[i]], pred_z[i])
        mae = mean_absolute_error(y_test[y_col[i]], pred_z[i])
        print("MSE: "+str(mse))
        print("MAE: "+str(mae))
        eval.append([mse, mae])
    return eval

"""### 4-2. 성능평가"""

# essay_types = ['주장', '설명글', '찬성반대', '대안제시', '글짓기']
# eval_list = []
'''
for e_type in essay_types:
    eval_list.append(evalution(e_type,'model_transformer_'+e_type,  'SentencePiece', 'Transformer', [20, 200, 2000], 20000))

for e in range(len(essay_types)):
    print(essay_types[e]+" 성능평가")
    print("AVG_MSE: "+str(np.mean(list(map(lambda x: x[0], eval_list[e])))))
    print("AVG_MAE: "+str(np.mean(list(map(lambda x: x[1], eval_list[e])))))
    print()
'''


